{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting UCI's Madelon Data Set Using Feature Selection and KNN, DecisionTrees, and RandomForests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Jordan Fritz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The small data set used for this project is UCI's Madelon data set. This synthetic data contains data points grouped into 32 different clusters which are placed at the vertices of a five dimensional hypercube. Each cluster is randomly assigned +1 or -1 and all data points in that cluster is then assigned this value. Along with these 5 informative features (vertices of the hypercube), 15 linear combinations of these features are also added, forming a set of 20 redundant and informative features. 480 additional \"noise\" features are added, creating a dataset of 500 features where only 20 are informative, and only 5 of these are the original 5 features.\n",
    "\n",
    "The larger data set is set up the same way, however, there are 500 additional noise features thus creating a 1000 feature dataset with only 20 informative and 5 \"original\" features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "The challenge is to develop a series of models for two purposes:\n",
    "\n",
    "1. for the purposes of identifying relevant features. \n",
    "2. for the purposes of generating predictions from the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric\n",
    "\n",
    "The metric that will be used for determining train and test scores for this data set is the accuracy test score. Since this is a classification problem, the accuracy metric will be determined by simply calculating the percentage of datapoints the model labeled correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Using the smaller UCI dataset (4400 observations, 500 features), dimensionality reduction was used to reduce the dataset from 500 features to 20 informative features using an $R^{2}$ score. These 20 features were further reduced using numerous feature reduction techniques (`SelectKBest` (SKB), Recursive Feature Elimination (`RFE`), `SelectFromModel`(SFM), `SelectPercentile`). Using at most the entire 20 feature dataset, and at least a 5 feature dataset found from one of these reduction techniques, various models were fitted using a grid search with cross-validation. The best test score (0.883) was returned when using a `KNeighborsClassifier` and 20 features. The optimal test score using only 5 features (0.865) was achieved using the `RFE` features found while fitting a `RandomForestClassifier` and using these features in a `RandomForestClassifier` model. Both of these models were fit using a dataset of 2000 observations and tested on a dataset of 600 observations.\n",
    "\n",
    "Using the large Madelon dataset (220000 observations, 1000 features), the above process was repeated to return the top 20 most informative features. These continued to be reduced using the methods described above. Using the model with the highest test score from the smaller dataset (`RandomForestClassifier` with `BaggingClassifier`) and features determined using the `RFE` of the `RandomForestClassifier`, a test score of 0.866 was accomplished using only 5 features and a train set and test set of size 21000 observations and 5000 observations, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why $R^{2}$ works\n",
    "\n",
    "For the following descriptions about how the features were reduced from 500 (or 1000) to 20, an $R^{2}$ score is calculated by first selecting a feature, fitting a regression model using the other 499 (or 999) features, and then returning an $R^{2}$ score depicting how close the other 499 features were to being able to predict the selected feature.\n",
    "\n",
    "The reason this process works for selecting important features is because the non-informative features are randomly assigned noise features. This means that these features cannot be predicted using the other features, and therefore will have very low (negative) $R^{2}$ scores. On the other hand, the informative features have linear relationships with other informative features, and therefore can be accurately predicted using the other features in the dataset. This would return a high $R^{2}$ score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UCI Madelon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce 500 features down to the 20 informative features, a function was created named `calculate_r2_for_feature` and placed in the `__init__.py` text file.\n",
    "\n",
    "In this function, the feature being explored becomes the target, and the other 499 features are used to determine this feature. Both a `KNeighborsRegressor` and a `DecisionTreeRegressor` are fit using a train set of the 499 features as the \"X\" value (predictors) and the chosen feature being the \"y\" value (target). A test set of similar data is used to return the $R^{2}$ score.\n",
    "\n",
    "After looping through all 500 features in the dataframe, each feature is assigned its corresponding $R^{2}$ score calculated from the function explained above. The following are the top 25 $R^{2}$ scores sorted by the `DecisionTreeRegressor` from a random sample of 10% of the data (three random samples of 10% of the data were taken and all three are included in the table below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>sample0_DecisionTree</th>\n",
       "      <th>sample0_KNeighbors</th>\n",
       "      <th>sample1_DecisionTree</th>\n",
       "      <th>sample1_KNeighbors</th>\n",
       "      <th>sample2_DecisionTree</th>\n",
       "      <th>sample2_KNeighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>153</td>\n",
       "      <td>0.967131</td>\n",
       "      <td>0.800221</td>\n",
       "      <td>0.970234</td>\n",
       "      <td>0.796493</td>\n",
       "      <td>0.972782</td>\n",
       "      <td>0.738970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>0.964194</td>\n",
       "      <td>0.870559</td>\n",
       "      <td>0.933456</td>\n",
       "      <td>0.870018</td>\n",
       "      <td>0.947502</td>\n",
       "      <td>0.876675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>442</td>\n",
       "      <td>0.954980</td>\n",
       "      <td>0.707660</td>\n",
       "      <td>0.943497</td>\n",
       "      <td>0.737102</td>\n",
       "      <td>0.934667</td>\n",
       "      <td>0.641258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>475</td>\n",
       "      <td>0.954109</td>\n",
       "      <td>0.761391</td>\n",
       "      <td>0.919846</td>\n",
       "      <td>0.841325</td>\n",
       "      <td>0.963652</td>\n",
       "      <td>0.732014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>493</td>\n",
       "      <td>0.952823</td>\n",
       "      <td>0.845834</td>\n",
       "      <td>0.936959</td>\n",
       "      <td>0.840637</td>\n",
       "      <td>0.869712</td>\n",
       "      <td>0.853006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>472</td>\n",
       "      <td>0.951643</td>\n",
       "      <td>0.811475</td>\n",
       "      <td>0.946205</td>\n",
       "      <td>0.805065</td>\n",
       "      <td>0.912156</td>\n",
       "      <td>0.808914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>0.944953</td>\n",
       "      <td>0.838523</td>\n",
       "      <td>0.954013</td>\n",
       "      <td>0.833933</td>\n",
       "      <td>0.966130</td>\n",
       "      <td>0.825389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>105</td>\n",
       "      <td>0.942681</td>\n",
       "      <td>0.749974</td>\n",
       "      <td>0.944329</td>\n",
       "      <td>0.704346</td>\n",
       "      <td>0.900712</td>\n",
       "      <td>0.759920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>336</td>\n",
       "      <td>0.940724</td>\n",
       "      <td>0.824993</td>\n",
       "      <td>0.948935</td>\n",
       "      <td>0.816537</td>\n",
       "      <td>0.926678</td>\n",
       "      <td>0.794521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>453</td>\n",
       "      <td>0.940601</td>\n",
       "      <td>0.851535</td>\n",
       "      <td>0.905721</td>\n",
       "      <td>0.867498</td>\n",
       "      <td>0.929027</td>\n",
       "      <td>0.888501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>28</td>\n",
       "      <td>0.939744</td>\n",
       "      <td>0.491834</td>\n",
       "      <td>0.949945</td>\n",
       "      <td>0.426479</td>\n",
       "      <td>0.950712</td>\n",
       "      <td>0.453083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>281</td>\n",
       "      <td>0.935742</td>\n",
       "      <td>0.849551</td>\n",
       "      <td>0.967380</td>\n",
       "      <td>0.846994</td>\n",
       "      <td>0.946156</td>\n",
       "      <td>0.826206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>378</td>\n",
       "      <td>0.931197</td>\n",
       "      <td>0.397125</td>\n",
       "      <td>0.937769</td>\n",
       "      <td>0.441486</td>\n",
       "      <td>0.963433</td>\n",
       "      <td>0.404046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>451</td>\n",
       "      <td>0.930366</td>\n",
       "      <td>0.497425</td>\n",
       "      <td>0.925214</td>\n",
       "      <td>0.425217</td>\n",
       "      <td>0.933558</td>\n",
       "      <td>0.490376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>48</td>\n",
       "      <td>0.925907</td>\n",
       "      <td>0.473653</td>\n",
       "      <td>0.907491</td>\n",
       "      <td>0.489082</td>\n",
       "      <td>0.955330</td>\n",
       "      <td>0.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>241</td>\n",
       "      <td>0.922280</td>\n",
       "      <td>0.783546</td>\n",
       "      <td>0.953034</td>\n",
       "      <td>0.829711</td>\n",
       "      <td>0.944485</td>\n",
       "      <td>0.759023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>318</td>\n",
       "      <td>0.917723</td>\n",
       "      <td>0.380825</td>\n",
       "      <td>0.935351</td>\n",
       "      <td>0.313931</td>\n",
       "      <td>0.937956</td>\n",
       "      <td>0.401857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>433</td>\n",
       "      <td>0.899667</td>\n",
       "      <td>0.844640</td>\n",
       "      <td>0.954365</td>\n",
       "      <td>0.832294</td>\n",
       "      <td>0.946624</td>\n",
       "      <td>0.798718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>338</td>\n",
       "      <td>0.775494</td>\n",
       "      <td>0.835246</td>\n",
       "      <td>0.782938</td>\n",
       "      <td>0.839469</td>\n",
       "      <td>0.733206</td>\n",
       "      <td>0.803634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>455</td>\n",
       "      <td>0.611537</td>\n",
       "      <td>0.836887</td>\n",
       "      <td>0.487147</td>\n",
       "      <td>0.868511</td>\n",
       "      <td>0.517069</td>\n",
       "      <td>0.832298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>291</td>\n",
       "      <td>-0.249839</td>\n",
       "      <td>-0.242241</td>\n",
       "      <td>-0.908320</td>\n",
       "      <td>-0.176619</td>\n",
       "      <td>-0.800044</td>\n",
       "      <td>-0.158394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>430</td>\n",
       "      <td>-0.392640</td>\n",
       "      <td>-0.257497</td>\n",
       "      <td>-1.286445</td>\n",
       "      <td>-0.092896</td>\n",
       "      <td>-1.004669</td>\n",
       "      <td>-0.237788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>225</td>\n",
       "      <td>-0.398883</td>\n",
       "      <td>-0.098002</td>\n",
       "      <td>-1.081159</td>\n",
       "      <td>0.040544</td>\n",
       "      <td>-1.299320</td>\n",
       "      <td>-0.228316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>447</td>\n",
       "      <td>-0.412036</td>\n",
       "      <td>-0.159824</td>\n",
       "      <td>-1.590800</td>\n",
       "      <td>-0.470881</td>\n",
       "      <td>-1.187044</td>\n",
       "      <td>-0.247717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.468673</td>\n",
       "      <td>-0.148561</td>\n",
       "      <td>-0.999532</td>\n",
       "      <td>-0.217317</td>\n",
       "      <td>-0.551354</td>\n",
       "      <td>-0.013223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature  sample0_DecisionTree  sample0_KNeighbors  sample1_DecisionTree  \\\n",
       "0       153              0.967131            0.800221              0.970234   \n",
       "1       128              0.964194            0.870559              0.933456   \n",
       "2       442              0.954980            0.707660              0.943497   \n",
       "3       475              0.954109            0.761391              0.919846   \n",
       "4       493              0.952823            0.845834              0.936959   \n",
       "5       472              0.951643            0.811475              0.946205   \n",
       "6        64              0.944953            0.838523              0.954013   \n",
       "7       105              0.942681            0.749974              0.944329   \n",
       "8       336              0.940724            0.824993              0.948935   \n",
       "9       453              0.940601            0.851535              0.905721   \n",
       "10       28              0.939744            0.491834              0.949945   \n",
       "11      281              0.935742            0.849551              0.967380   \n",
       "12      378              0.931197            0.397125              0.937769   \n",
       "13      451              0.930366            0.497425              0.925214   \n",
       "14       48              0.925907            0.473653              0.907491   \n",
       "15      241              0.922280            0.783546              0.953034   \n",
       "16      318              0.917723            0.380825              0.935351   \n",
       "17      433              0.899667            0.844640              0.954365   \n",
       "18      338              0.775494            0.835246              0.782938   \n",
       "19      455              0.611537            0.836887              0.487147   \n",
       "20      291             -0.249839           -0.242241             -0.908320   \n",
       "21      430             -0.392640           -0.257497             -1.286445   \n",
       "22      225             -0.398883           -0.098002             -1.081159   \n",
       "23      447             -0.412036           -0.159824             -1.590800   \n",
       "24        1             -0.468673           -0.148561             -0.999532   \n",
       "\n",
       "    sample1_KNeighbors  sample2_DecisionTree  sample2_KNeighbors  \n",
       "0             0.796493              0.972782            0.738970  \n",
       "1             0.870018              0.947502            0.876675  \n",
       "2             0.737102              0.934667            0.641258  \n",
       "3             0.841325              0.963652            0.732014  \n",
       "4             0.840637              0.869712            0.853006  \n",
       "5             0.805065              0.912156            0.808914  \n",
       "6             0.833933              0.966130            0.825389  \n",
       "7             0.704346              0.900712            0.759920  \n",
       "8             0.816537              0.926678            0.794521  \n",
       "9             0.867498              0.929027            0.888501  \n",
       "10            0.426479              0.950712            0.453083  \n",
       "11            0.846994              0.946156            0.826206  \n",
       "12            0.441486              0.963433            0.404046  \n",
       "13            0.425217              0.933558            0.490376  \n",
       "14            0.489082              0.955330            0.484100  \n",
       "15            0.829711              0.944485            0.759023  \n",
       "16            0.313931              0.937956            0.401857  \n",
       "17            0.832294              0.946624            0.798718  \n",
       "18            0.839469              0.733206            0.803634  \n",
       "19            0.868511              0.517069            0.832298  \n",
       "20           -0.176619             -0.800044           -0.158394  \n",
       "21           -0.092896             -1.004669           -0.237788  \n",
       "22            0.040544             -1.299320           -0.228316  \n",
       "23           -0.470881             -1.187044           -0.247717  \n",
       "24           -0.217317             -0.551354           -0.013223  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "r2_df = pd.read_csv('files/r2_dataframe')\n",
    "r2_df = r2_df.rename(columns={'Unnamed: 0':'Feature'})\n",
    "display(r2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, exactly 20 features have positive $R^{2}$ values for both models and all three samples, and all other features have negative $R^{2}$ values for both models and all three samples. This makes it extremely likely that these 20 features are the 20 informative features that were being searched for, and the rest of the analysis is done using only these 20 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these 20 features were determined, `SelectKBest`, `RFE`, `SelectFromModel`, and `SelectPercentile` were used to furthefeature reduction.\n",
    "\n",
    "**`SelectKBest`**: Selects and returns the top K features in order of importance. For this dataset, K = 5 returned the features 64, 128, 241, 336, and 475 when run on the full 2000 observation dataset. However, for a population size of 2000, a 90% confidence interval with a 1% margin of error requires only 1656 observations. When all populations between 1650 and 2000 are used with a step size of 50 to calculate the features, the following are the percentage of times that each feature appears in the top 5:\n",
    "\n",
    "64: 100%\n",
    "\n",
    "241: 100%\n",
    "\n",
    "336: 100%\n",
    "\n",
    "475: 100%\n",
    "\n",
    "128: 50%\n",
    "\n",
    "338: 37.5%\n",
    "\n",
    "472: 12.5%\n",
    "\n",
    "These findings somewhat mirror the findings when run on the full dataset, so the `SelectKBest` features were determined to be 64, 128, 241, 336, and 475.\n",
    "\n",
    "**`RFE`**: Eliminates features based on which combination of `n_features_to_select` gives the highest test score for a particular model. For this analysis, `RFE` was used with `DecisionTreeClassifier`, `LogisiticRegression`, and `RandomForestClassifier` with `n_features_to_select` = 5. The features that were returned were as follows:\n",
    "\n",
    "`DecisionTreeClassifier`: 48, 105, 338, 442, 475\n",
    "\n",
    "`LogisticRegression`: 28, 128, 318, 442, 451\n",
    "\n",
    "`RandomForestClassifier`: 48, 105, 318, 338, 475\n",
    "\n",
    "**`SelectFromModel`**: Selects the features that are the most important for a specific model based on importance weights. This can be more or less than 5 features. For this analysis, the models used were `DecisionTreeClassifier`, `LogisiticRegression`, and `RandomForestClassifier`. The `SelectFromModel` returned the following features:\n",
    "\n",
    "`DecisionTreeClassifier`: 28, 48, 105, 153, 338, 378, 442, 451, 453, 475\n",
    "\n",
    "`LogisticRegression`: 28, 153, 318, 338, 433, 442, 451, 475\n",
    "\n",
    "`RandomForestClassifier`: 48, 105, 128, 241, 281, 318, 338, 378, 433\n",
    "\n",
    "**`SelectPercentile`**: Selects features according to a percentile of the highest scores. Used a percentile of 25 for this analysis, and the following features were returned: 475, 241, 336, 64, 128. These are returned in order of their appearances, and matches the `SelectKBest` model exactly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A union of all of these features creates a list of features: 28, 48, 64, 105, 128, 153, 241, 318, 336, 338, 378, 433, 442, 451, 453, and 475. These features were then used to fit an Ordinary Least Squares (`OLS`) model and the p-values for each feature is returned with the null hypothesis being stated as \"the feature is not important in determining the target variable\". The majority of these features had a p-value less than $10^{-4}$, however five of the features had large p-values. The following were the features with large p-values their corresponding p-value:\n",
    "\n",
    "28 - 0.8731824724915853\n",
    "\n",
    "153 - 0.13095957920122445\n",
    "\n",
    "318 - 0.8338883796872708\n",
    "\n",
    "433 - 0.1825285610504542\n",
    "\n",
    "451 - 0.6041125129060525"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Larger Madelon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a similar process as outlined in the UCI Madelon Dataset,  first only 4000 observations were used to calculate $R^{2}$ values for all 1000 features. The table below shows the top 25 $R^{2}$ values. Once again, there are exactly 20 positive $R^{2}$ values, while the rest of the 980 features all have negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>sample0_DecisionTree</th>\n",
       "      <th>sample0_KNeighbors</th>\n",
       "      <th>sample1_DecisionTree</th>\n",
       "      <th>sample1_KNeighbors</th>\n",
       "      <th>sample2_DecisionTree</th>\n",
       "      <th>sample2_KNeighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>639</td>\n",
       "      <td>0.927408</td>\n",
       "      <td>0.765546</td>\n",
       "      <td>0.928160</td>\n",
       "      <td>0.736336</td>\n",
       "      <td>0.865986</td>\n",
       "      <td>0.654629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>956</td>\n",
       "      <td>0.886172</td>\n",
       "      <td>0.762094</td>\n",
       "      <td>0.901443</td>\n",
       "      <td>0.721654</td>\n",
       "      <td>0.873643</td>\n",
       "      <td>0.648066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>867</td>\n",
       "      <td>0.819643</td>\n",
       "      <td>0.616547</td>\n",
       "      <td>0.772777</td>\n",
       "      <td>0.636942</td>\n",
       "      <td>0.695628</td>\n",
       "      <td>0.585044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>336</td>\n",
       "      <td>0.781835</td>\n",
       "      <td>0.631073</td>\n",
       "      <td>0.752961</td>\n",
       "      <td>0.696783</td>\n",
       "      <td>0.672042</td>\n",
       "      <td>0.522090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>920</td>\n",
       "      <td>0.749918</td>\n",
       "      <td>0.493926</td>\n",
       "      <td>0.694970</td>\n",
       "      <td>0.419661</td>\n",
       "      <td>0.662646</td>\n",
       "      <td>0.527689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>701</td>\n",
       "      <td>0.745191</td>\n",
       "      <td>0.516612</td>\n",
       "      <td>0.780464</td>\n",
       "      <td>0.576887</td>\n",
       "      <td>0.670117</td>\n",
       "      <td>0.547489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>269</td>\n",
       "      <td>0.739760</td>\n",
       "      <td>0.485213</td>\n",
       "      <td>0.708770</td>\n",
       "      <td>0.662076</td>\n",
       "      <td>0.837314</td>\n",
       "      <td>0.608524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>504</td>\n",
       "      <td>0.723879</td>\n",
       "      <td>0.542038</td>\n",
       "      <td>0.482788</td>\n",
       "      <td>0.452808</td>\n",
       "      <td>0.437115</td>\n",
       "      <td>0.356188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>341</td>\n",
       "      <td>0.711988</td>\n",
       "      <td>0.670392</td>\n",
       "      <td>0.778390</td>\n",
       "      <td>0.711253</td>\n",
       "      <td>0.747792</td>\n",
       "      <td>0.703786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>395</td>\n",
       "      <td>0.704585</td>\n",
       "      <td>0.552216</td>\n",
       "      <td>0.727241</td>\n",
       "      <td>0.599737</td>\n",
       "      <td>0.746817</td>\n",
       "      <td>0.624806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>724</td>\n",
       "      <td>0.693780</td>\n",
       "      <td>0.561768</td>\n",
       "      <td>0.694918</td>\n",
       "      <td>0.597765</td>\n",
       "      <td>0.722448</td>\n",
       "      <td>0.617379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>315</td>\n",
       "      <td>0.689037</td>\n",
       "      <td>0.600497</td>\n",
       "      <td>0.762291</td>\n",
       "      <td>0.653811</td>\n",
       "      <td>0.806019</td>\n",
       "      <td>0.637660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>829</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.595766</td>\n",
       "      <td>0.549854</td>\n",
       "      <td>0.484093</td>\n",
       "      <td>0.553949</td>\n",
       "      <td>0.418499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>736</td>\n",
       "      <td>0.658151</td>\n",
       "      <td>0.469071</td>\n",
       "      <td>0.588398</td>\n",
       "      <td>0.541504</td>\n",
       "      <td>0.562101</td>\n",
       "      <td>0.386677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>257</td>\n",
       "      <td>0.606124</td>\n",
       "      <td>0.604212</td>\n",
       "      <td>0.686228</td>\n",
       "      <td>0.606748</td>\n",
       "      <td>0.705658</td>\n",
       "      <td>0.578984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>769</td>\n",
       "      <td>0.535470</td>\n",
       "      <td>0.500617</td>\n",
       "      <td>0.638929</td>\n",
       "      <td>0.528797</td>\n",
       "      <td>0.583860</td>\n",
       "      <td>0.410272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>526</td>\n",
       "      <td>0.473975</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>0.499838</td>\n",
       "      <td>0.508901</td>\n",
       "      <td>0.419409</td>\n",
       "      <td>0.505563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>808</td>\n",
       "      <td>0.442773</td>\n",
       "      <td>0.472452</td>\n",
       "      <td>0.610752</td>\n",
       "      <td>0.500485</td>\n",
       "      <td>0.658955</td>\n",
       "      <td>0.414743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>681</td>\n",
       "      <td>0.422852</td>\n",
       "      <td>0.329518</td>\n",
       "      <td>0.596078</td>\n",
       "      <td>0.318474</td>\n",
       "      <td>0.567058</td>\n",
       "      <td>0.141681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>308</td>\n",
       "      <td>0.377005</td>\n",
       "      <td>0.282912</td>\n",
       "      <td>0.569598</td>\n",
       "      <td>0.304817</td>\n",
       "      <td>0.509340</td>\n",
       "      <td>0.244508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>463</td>\n",
       "      <td>-0.304912</td>\n",
       "      <td>-0.072661</td>\n",
       "      <td>-1.352521</td>\n",
       "      <td>-0.192762</td>\n",
       "      <td>-1.111292</td>\n",
       "      <td>-0.230748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>502</td>\n",
       "      <td>-0.356546</td>\n",
       "      <td>-0.272216</td>\n",
       "      <td>-1.585285</td>\n",
       "      <td>-0.225520</td>\n",
       "      <td>-0.763924</td>\n",
       "      <td>-0.192816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>337</td>\n",
       "      <td>-0.385433</td>\n",
       "      <td>-0.133136</td>\n",
       "      <td>-1.179454</td>\n",
       "      <td>-0.352835</td>\n",
       "      <td>-1.433839</td>\n",
       "      <td>-0.158929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>542</td>\n",
       "      <td>-0.413927</td>\n",
       "      <td>-0.350349</td>\n",
       "      <td>-1.037661</td>\n",
       "      <td>-0.164488</td>\n",
       "      <td>-1.564160</td>\n",
       "      <td>-0.333289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>753</td>\n",
       "      <td>-0.443608</td>\n",
       "      <td>-0.101489</td>\n",
       "      <td>-0.892912</td>\n",
       "      <td>-0.114874</td>\n",
       "      <td>-1.061632</td>\n",
       "      <td>-0.220567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature  sample0_DecisionTree  sample0_KNeighbors  sample1_DecisionTree  \\\n",
       "0       639              0.927408            0.765546              0.928160   \n",
       "1       956              0.886172            0.762094              0.901443   \n",
       "2       867              0.819643            0.616547              0.772777   \n",
       "3       336              0.781835            0.631073              0.752961   \n",
       "4       920              0.749918            0.493926              0.694970   \n",
       "5       701              0.745191            0.516612              0.780464   \n",
       "6       269              0.739760            0.485213              0.708770   \n",
       "7       504              0.723879            0.542038              0.482788   \n",
       "8       341              0.711988            0.670392              0.778390   \n",
       "9       395              0.704585            0.552216              0.727241   \n",
       "10      724              0.693780            0.561768              0.694918   \n",
       "11      315              0.689037            0.600497              0.762291   \n",
       "12      829              0.662500            0.595766              0.549854   \n",
       "13      736              0.658151            0.469071              0.588398   \n",
       "14      257              0.606124            0.604212              0.686228   \n",
       "15      769              0.535470            0.500617              0.638929   \n",
       "16      526              0.473975            0.565200              0.499838   \n",
       "17      808              0.442773            0.472452              0.610752   \n",
       "18      681              0.422852            0.329518              0.596078   \n",
       "19      308              0.377005            0.282912              0.569598   \n",
       "20      463             -0.304912           -0.072661             -1.352521   \n",
       "21      502             -0.356546           -0.272216             -1.585285   \n",
       "22      337             -0.385433           -0.133136             -1.179454   \n",
       "23      542             -0.413927           -0.350349             -1.037661   \n",
       "24      753             -0.443608           -0.101489             -0.892912   \n",
       "\n",
       "    sample1_KNeighbors  sample2_DecisionTree  sample2_KNeighbors  \n",
       "0             0.736336              0.865986            0.654629  \n",
       "1             0.721654              0.873643            0.648066  \n",
       "2             0.636942              0.695628            0.585044  \n",
       "3             0.696783              0.672042            0.522090  \n",
       "4             0.419661              0.662646            0.527689  \n",
       "5             0.576887              0.670117            0.547489  \n",
       "6             0.662076              0.837314            0.608524  \n",
       "7             0.452808              0.437115            0.356188  \n",
       "8             0.711253              0.747792            0.703786  \n",
       "9             0.599737              0.746817            0.624806  \n",
       "10            0.597765              0.722448            0.617379  \n",
       "11            0.653811              0.806019            0.637660  \n",
       "12            0.484093              0.553949            0.418499  \n",
       "13            0.541504              0.562101            0.386677  \n",
       "14            0.606748              0.705658            0.578984  \n",
       "15            0.528797              0.583860            0.410272  \n",
       "16            0.508901              0.419409            0.505563  \n",
       "17            0.500485              0.658955            0.414743  \n",
       "18            0.318474              0.567058            0.141681  \n",
       "19            0.304817              0.509340            0.244508  \n",
       "20           -0.192762             -1.111292           -0.230748  \n",
       "21           -0.225520             -0.763924           -0.192816  \n",
       "22           -0.352835             -1.433839           -0.158929  \n",
       "23           -0.164488             -1.564160           -0.333289  \n",
       "24           -0.114874             -1.061632           -0.220567  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "big_r2_df = pd.read_csv('files/bigdata_r2')\n",
    "big_r2_df = big_r2_df.rename(columns={'Unnamed: 0':'Feature'})\n",
    "display(big_r2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these 20 features were determined, the server could handle loading in 60000 observations of the 20 features, rather than the original 4000 observations of 1000 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these 20 features, SelectKBest, RFE, SelectFromModel, and SelectPercentile were used to further feature reduction.\n",
    "\n",
    "**`SelectKBest`**: Using three different samples of 20000 observations and K = 5, the features 269, 341, 681, 701, and 920 were selected for all three samples.\n",
    "\n",
    "**`RFE`**: For this analysis, `RFE` was used with `DecisionTreeClassifier`, `LogisiticRegression`, and `RandomForestClassifier` with `n_features_to_select` = 5. This model was run on only one of the 20000 observation samples. The features that were returned were as follows:\n",
    "\n",
    "`DecisionTreeClassifier`: 269, 308, 724, 769, 829\n",
    "\n",
    "`LogisticRegression`: 269, 504, 681, 829, 920\n",
    "\n",
    "`RandomForestClassifier`: 269, 308, 395, 808, 920\n",
    "\n",
    "**`SelectFromModel`**: For this analysis, the models used were `DecisionTreeClassifier`, `LogisiticRegression`, and `RandomForestClassifier`. This model was run on only one of the 20000 observation samples. The `SelectFromModel` returned the following features:\n",
    "\n",
    "`DecisionTreeClassifier`: 269, 308, 681, 724, 736, 769, 808, 829, 920\n",
    "\n",
    "`LogisticRegression`: 257, 269, 308, 341, 504, 681, 701, 769, 808, 829, 920\n",
    "\n",
    "`RandomForestClassifier`: 269, 308, 395, 504, 681, 724, 769, 808, 920, 956\n",
    "\n",
    "**`SelectPercentile`**: Used a percentile of 25 for this analysis and a single sample of 20000 observations. The following features were returned: 269, 701, 681, 920, 341. These are returned in order of their appearances, and they match the `SelectKBest` model exactly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A union of all of these features creates a list of features: 257, 269, 308, 341, 504, 681, 701, 724, 736, 769, 808, 829, and 920."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UCI Madelon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking was accomplished using naive fits for four unique models using raw data from the smaller UCI Madelon data set of 2000 train observations and 600 test. Only the 20 features found from the $R^{2}$ analysis were used. The four models used for benchmarking were `KNeighborsClassifier`, `DecisionTreeClassifier`, `RandomForestClassifier`, and `LogisticRegression`. The train and test scores were as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>LogisticRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train Score</td>\n",
       "      <td>0.838333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test Score</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.522222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                KNeighborsClassifier  DecisionTreeClassifier  \\\n",
       "0  Train Score              0.838333                1.000000   \n",
       "1   Test Score              0.744444                0.722222   \n",
       "\n",
       "   RandomForestClassifier  LogisticRegression  \n",
       "0                0.976667            0.655000  \n",
       "1                0.683333            0.522222  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_scores = pd.read_csv('files/raw_benchmark_scores')\n",
    "raw_scores = raw_scores.rename(columns = {'Unnamed: 0': ''})\n",
    "display(raw_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, the highest naive fit benchmark score occurs in the `KNeighborsClassifier` where the score is 0.744."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Larger Madelon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking was accomplished using naive fits for four unique models using both raw and scaled data from the larger Madelon data set of 20000 train observations and 2000 test. Only the 20 features found from the $R^{2}$ analysis were used. The four models used for benchmarking were `KNeighborsClassifier`, `DecisionTreeClassifier`, `RandomForestClassifier`, and `LogisticRegression`. The train and test scores were as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>LogisticRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train Score</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987333</td>\n",
       "      <td>0.591167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test Score</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                KNeighborsClassifier  DecisionTreeClassifier  \\\n",
       "0  Train Score              0.843000                     1.0   \n",
       "1   Test Score              0.796667                     0.7   \n",
       "\n",
       "   RandomForestClassifier  LogisticRegression  \n",
       "0                0.987333            0.591167  \n",
       "1                0.760000            0.571667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_scores_bigdata = pd.read_csv('files/raw_benchmark_bigdata')\n",
    "raw_scores_bigdata = raw_scores_bigdata.rename(columns = {'Unnamed: 0': ''})\n",
    "display(raw_scores_bigdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the table above, the highest benchmark score is 0.797 and is achieved using the `KNeighborsClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UCI Madelon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using various combinations of features and hyperparameters, the three models that were used were `KNeighborsClassifier`, `DecisionTreeClassifier`, and `RandomForestClassifier`. The `DecisionTreeClassifier` and the `RandomForestClassifier` were used in combination with a `BaggingClassifier` as well for some of its fits. The scores below represent various models and their fits using particular features and hyperparameters. All hyperparameters were determined using `GridSearchCV`, and all models were fit using a train set of 2000 observations and tested using a test set of 600 observations. All data was scaled prior to training and testing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. `KNeighborsClassifier(algorithm = 'auto', n_neighbors = 6, weights = 'distance')` \n",
    "- 20 features: `[28, 48, 64, 105, 128, 153, 241, 281, 318, 336, 338, 378, 433, 442, 451, 453, 455, 472, 475, 493]` \n",
    "- *Train score = 1.0* | *Test score = 0.883*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. `KNeighborsClassifier(algorithm = 'auto', n_neighbors = 17, weights = 'distance')`\n",
    "- 5 features selected using `SelectKBest`: `[64, 128, 241, 336, 475]` \n",
    "- *Train score = 1.0* | *Test score = 0.728*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### 3. `DecisionTreeClassifier(criterion = 'entropy', max_depth = None, max_features = 0.01, min_samples_split = 0.01, random_state = 42)` \n",
    "- 5 features selected using `SelectKBest`: `[64, 128, 241, 336, 475]`\n",
    "- *Train score = 1.0* | *Test score = 0.632*\n",
    "\n",
    "---\n",
    "- `BaggingClassifier(model, n_estimators = 15, max_features = 5, random_state = 42)`\n",
    "- 5 features selected using `SelectKBest`: `[64, 128, 241, 336, 475]`\n",
    "- *Train score = 0.860* | *Test score = 0.708*\n",
    "\n",
    "---\n",
    "- `BaggingClassifier(model, n_estimators = 40, max_features = 5, random_state = 42)`\n",
    "- 5 features selected using `RFE` fit on `DecisionTreeClassifier`: `[48, 105, 338, 442, 475]`\n",
    "- *Train score = 0.937* | *Test score = 0.835*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4. `DecisionTreeClassifier(criterion = 'gini', max_depth = None, max_features = 0.01, min_samples_split = 0.01, random_state = 42)`\n",
    "- 5 features selected using `RFE` fit on `DecisionTreeClassifier`: `[48, 105, 338, 442, 475]`\n",
    "- *Train score = 0.857* | *Test score = 0.717*\n",
    "\n",
    "---\n",
    "- 10 features selected using `SelectFromModel` fit on `DecisionTreeClassifier`: `[28, 48, 105, 153, 338, 378, 442, 451, 453, 475]`\n",
    "- *Train score = 0.865* | *Test score = 0.752*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. `RandomForestClassifier(criterion = 'entropy', max_depth = None, max_features = 'auto', n_estimators = 100, random_state = 42)`\n",
    "- 5 features selected using `RFE` fit on `RandomForestClassifier`: `[105, 241, 318, 338, 378]`\n",
    "- *Train score = 1.0* | *Test score = 0.865*\n",
    "\n",
    "---\n",
    "- `BaggingClassifier(model, max_features = 5, n_estimators = 45, max_samples = 0.8, random_state = 42)`\n",
    "- 5 featues selected using `RFE` fit on `RandomForestClassifier`: `[105, 241, 318, 338, 378]`\n",
    "- *Train score = 0.978* | *Test score = 0.850*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6.  `RandomForestClassifier(criterion = 'entropy', max_depth = None, max_features = 'auto', n_estimators = 350, random_state = 42)`\n",
    "- 11 features selected following the p-value test: `[48, 64, 105, 128, 241, 336, 338, 378, 442, 453, 475]`\n",
    "- *Train score = 1.0* | *Test score = 0.87*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest test score using only 5 features was achieved using the `RandomForestClassifier` in model 5. The higest test score overall was achieved using all 20 features on the `KNeighborsClassifier` in model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Larger Madelon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using the information collected above, high scoring models from the smaller UCI Madelon dataset were used on the large dataset in order to achieve high test scores. The scores below represent various models and their fits using particular features and hyperparameters. All hyperparameters were determined using GridSearchCV for the smaller dataset and were not re-calibrated using the larger data set. All models were fit using a train set of 21000 observations and tested using three different test sets of 5000 observations. These test scores were then averaged out to obtain the final test score. All data was scaled prior to training and testing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. `KNeighborsClassifier(algorithm = 'auto', n_neighbors = 6, weights = 'distance')` \n",
    "- 20 features: `[257, 269, 308, 315, 336, 341, 395, 504, 526, 639, 681, 701, 724, 736, 769, 808, 829, 867, 920, 956]` \n",
    "- *Train score = 1.0* | *Test score = 0.891*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. `RandomForestClassifier(criterion = 'entropy', max_depth = None, max_features = 'auto', n_estimators = 100, random_state = 42)`\n",
    "- 5 features selected using `RFE` fit on `RandomForestClassifier`: `[269, 308, 395, 808, 920]`\n",
    "- *Train score = 1.0* | *Test score = 0.802*\n",
    "\n",
    "---\n",
    "- `BaggingClassifier(model, max_samples = .8, max_features = 5, random_state=42)`\n",
    "- 5 features selected using `RFE` fit on `RandomForestClassifier`: `[269, 308, 395, 808, 920]`\n",
    "- *Train score = 0.963* | *Test score = 0.866*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
